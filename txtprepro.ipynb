{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deviding the dataset into 10 parts\n",
    "part_1 = df.iloc[0:68130,:]\n",
    "part_2 = df.iloc[68130:136260,:]\n",
    "part_3 = df.iloc[136260:204390,:]\n",
    "part_4 = df.iloc[204390:272520,:]\n",
    "part_5 = df.iloc[272520:340650,:]\n",
    "part_6 = df.iloc[340650:408780,:]\n",
    "part_7 = df.iloc[408780:476910,:]\n",
    "part_8 = df.iloc[476910:545040,:]\n",
    "part_9 = df.iloc[545040:613170,:]\n",
    "part_10 = df.iloc[613170:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(tokens):\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    wn_pos_tags = {'N': 'n', 'V': 'v', 'R': 'r', 'J': 'a'}\n",
    "    \n",
    "    # Lemmatize each word based on its POS tag\n",
    "    lemmas = []\n",
    "    for token, pos in pos_tags:\n",
    "        pos = wn_pos_tags.get(pos[0].upper(), 'n')  # If the POS tag is not recognized, default to noun (n)\n",
    "        lemma = lemmatizer.lemmatize(token, pos=pos)\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Processing in a single code\n",
    "words = set(nltk.corpus.words.words())\n",
    "def text_cleaner(blogs):\n",
    "    blogs = \" \".join(filter(lambda x: x[0]!= '@' , blogs.split())) # removes any word which starts with @.\n",
    "    blogs = re.sub('[^0-9a-zA-Z]', ' ', blogs) # substitute all non-alphabetic characters in the string with spaces.\n",
    "    blogs = blogs.lower() # lowercasing\n",
    "    blogs = re.sub(' +', ' ', blogs).strip() # replacing more than one space with 1 space and remove any leading and trailing space.\n",
    "    blogs = nltk.wordpunct_tokenize(blogs)\n",
    "    blogs = [word for word in blogs if not word.isdigit()]\n",
    "    blogs = [word for word in blogs if word.lower() in words or not word.isalpha()]\n",
    "    blogs = [word for word in blogs if word not in string.punctuation] # filtering out any punctuation marks from the string.\n",
    "    blogs = [word for word in blogs if not word in set(stopwords.words('english'))] # Keeping only the non-stop words in the string\n",
    "    blogs = lemmatize(blogs)\n",
    "    blogs = \" \".join(blogs)\n",
    "    return blogs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_1['clean_blog']=part_1['blog'].apply(text_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_2['clean_blog']=part_2['blog'].apply(text_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_3['clean_blog']=part_3['blog'].apply(text_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_4['clean_blog']=part_4['blog'].apply(text_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_5['clean_blog']=part_5['blog'].apply(text_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_6['clean_blog']=part_6['blog'].apply(text_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_7['clean_blog']=part_7['blog'].apply(text_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_8['clean_blog']=part_8['blog'].apply(text_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_9['clean_blog']=part_9['blog'].apply(text_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_10['clean_blog']=part_10['blog'].apply(text_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandf = pd.concat([part_1, part_2, part_3, part_4, part_5, part_6, part_7, part_8, part_9, part_10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = []\n",
    "for i in cleandf['clean_blog']:\n",
    "    x =  word_tokenize(i)\n",
    "    tokenized_text.append(x)\n",
    "cleandf['tokenized'] = tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandf.to_csv(r'cleandf.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
